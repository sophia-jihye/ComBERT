{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-resource setting: Fine-tuning data sizes are [50, 100, 200, 300, 400]\n",
    "* For test set containing company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re, copy\n",
    "from glob import glob\n",
    "from reports import get_average_classification_report\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "from functools import reduce\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Araci_CM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Araci_NoPT_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Araci_SM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Araci_WWM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_BERT_CM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_BERT_NoPT_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_BERT_SM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_BERT_WWM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_SECBERT_CM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_SECBERT_NoPT_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_SECBERT_SM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_SECBERT_WWM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Yang_CM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Yang_NoPT_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Yang_SM_epoch7_seed4',\n",
       " 'C:\\\\DATA\\\\ComBERT\\\\results_spamFiltering\\\\testCompanyName=y_Yang_WWM_epoch7_seed4']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_trained_with_nums = [50, 100, 200, 300, 400]\n",
    "dirnames = sorted(glob(os.path.join('C:\\DATA\\ComBERT', 'results_spamFiltering', 'testCompanyName=y_*_seed4')))\n",
    "print(len(dirnames))\n",
    "dirnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1-score\n",
    "* All values are macro average values.\n",
    "\n",
    "* Download the table below in an Excel format, then add a \"range\" row that indicates the maximum -- maximum value.\n",
    "    * The term “range” in the table indicates the minimum and maximum values of the average scores obtained through a model.\n",
    "    * Based on the minimum values indicated in the “range” row, SEC-BERT shows robust performance regardless of the post-training methods, recording the highest value among the minimum performance scores of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:02<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created results\\LowResource_TestSetContainingCompanyNames_PrecisionRecallF1score.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Araci_CM</th>\n",
       "      <th>Araci_NoPT</th>\n",
       "      <th>Araci_SM</th>\n",
       "      <th>Araci_WWM</th>\n",
       "      <th>BERT_CM</th>\n",
       "      <th>BERT_NoPT</th>\n",
       "      <th>BERT_SM</th>\n",
       "      <th>BERT_WWM</th>\n",
       "      <th>SECBERT_CM</th>\n",
       "      <th>SECBERT_NoPT</th>\n",
       "      <th>SECBERT_SM</th>\n",
       "      <th>SECBERT_WWM</th>\n",
       "      <th>Yang_CM</th>\n",
       "      <th>Yang_NoPT</th>\n",
       "      <th>Yang_SM</th>\n",
       "      <th>Yang_WWM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FinetuningSize_Measure</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1-score_050</th>\n",
       "      <td>49.48</td>\n",
       "      <td>28.84</td>\n",
       "      <td>52.55</td>\n",
       "      <td>35.48</td>\n",
       "      <td>47.65</td>\n",
       "      <td>49.15</td>\n",
       "      <td>42.11</td>\n",
       "      <td>49.86</td>\n",
       "      <td>53.65</td>\n",
       "      <td>50.84</td>\n",
       "      <td>53.45</td>\n",
       "      <td>45.57</td>\n",
       "      <td>55.05</td>\n",
       "      <td>42.59</td>\n",
       "      <td>51.43</td>\n",
       "      <td>43.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score_100</th>\n",
       "      <td>59.77</td>\n",
       "      <td>44.39</td>\n",
       "      <td>65.34</td>\n",
       "      <td>40.29</td>\n",
       "      <td>63.09</td>\n",
       "      <td>62.54</td>\n",
       "      <td>56.34</td>\n",
       "      <td>51.50</td>\n",
       "      <td>62.35</td>\n",
       "      <td>64.93</td>\n",
       "      <td>68.54</td>\n",
       "      <td>60.38</td>\n",
       "      <td>67.57</td>\n",
       "      <td>62.33</td>\n",
       "      <td>66.55</td>\n",
       "      <td>59.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score_200</th>\n",
       "      <td>76.83</td>\n",
       "      <td>50.56</td>\n",
       "      <td>79.38</td>\n",
       "      <td>59.83</td>\n",
       "      <td>73.02</td>\n",
       "      <td>75.58</td>\n",
       "      <td>76.98</td>\n",
       "      <td>58.94</td>\n",
       "      <td>78.94</td>\n",
       "      <td>76.07</td>\n",
       "      <td>75.23</td>\n",
       "      <td>68.33</td>\n",
       "      <td>77.28</td>\n",
       "      <td>74.16</td>\n",
       "      <td>79.03</td>\n",
       "      <td>68.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score_300</th>\n",
       "      <td>79.25</td>\n",
       "      <td>61.43</td>\n",
       "      <td>79.32</td>\n",
       "      <td>67.50</td>\n",
       "      <td>79.12</td>\n",
       "      <td>78.51</td>\n",
       "      <td>78.37</td>\n",
       "      <td>65.91</td>\n",
       "      <td>80.52</td>\n",
       "      <td>77.19</td>\n",
       "      <td>76.84</td>\n",
       "      <td>75.89</td>\n",
       "      <td>80.12</td>\n",
       "      <td>79.07</td>\n",
       "      <td>80.81</td>\n",
       "      <td>74.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score_400</th>\n",
       "      <td>76.70</td>\n",
       "      <td>64.67</td>\n",
       "      <td>77.93</td>\n",
       "      <td>70.64</td>\n",
       "      <td>78.50</td>\n",
       "      <td>76.60</td>\n",
       "      <td>79.26</td>\n",
       "      <td>68.60</td>\n",
       "      <td>80.34</td>\n",
       "      <td>77.43</td>\n",
       "      <td>78.82</td>\n",
       "      <td>76.87</td>\n",
       "      <td>79.69</td>\n",
       "      <td>79.43</td>\n",
       "      <td>79.02</td>\n",
       "      <td>75.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score_Average</th>\n",
       "      <td>68.40</td>\n",
       "      <td>49.98</td>\n",
       "      <td>70.91</td>\n",
       "      <td>54.75</td>\n",
       "      <td>68.28</td>\n",
       "      <td>68.48</td>\n",
       "      <td>66.61</td>\n",
       "      <td>58.96</td>\n",
       "      <td>71.16</td>\n",
       "      <td>69.29</td>\n",
       "      <td>70.57</td>\n",
       "      <td>65.41</td>\n",
       "      <td>71.94</td>\n",
       "      <td>67.52</td>\n",
       "      <td>71.37</td>\n",
       "      <td>64.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_050</th>\n",
       "      <td>58.22</td>\n",
       "      <td>30.29</td>\n",
       "      <td>69.26</td>\n",
       "      <td>43.14</td>\n",
       "      <td>54.03</td>\n",
       "      <td>57.79</td>\n",
       "      <td>46.99</td>\n",
       "      <td>54.18</td>\n",
       "      <td>62.53</td>\n",
       "      <td>60.62</td>\n",
       "      <td>65.69</td>\n",
       "      <td>51.65</td>\n",
       "      <td>63.57</td>\n",
       "      <td>47.05</td>\n",
       "      <td>60.94</td>\n",
       "      <td>55.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_100</th>\n",
       "      <td>66.19</td>\n",
       "      <td>45.02</td>\n",
       "      <td>75.53</td>\n",
       "      <td>49.42</td>\n",
       "      <td>69.05</td>\n",
       "      <td>65.59</td>\n",
       "      <td>67.76</td>\n",
       "      <td>54.61</td>\n",
       "      <td>74.10</td>\n",
       "      <td>68.58</td>\n",
       "      <td>73.75</td>\n",
       "      <td>62.46</td>\n",
       "      <td>70.67</td>\n",
       "      <td>63.69</td>\n",
       "      <td>70.72</td>\n",
       "      <td>65.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_200</th>\n",
       "      <td>79.22</td>\n",
       "      <td>51.53</td>\n",
       "      <td>81.59</td>\n",
       "      <td>61.54</td>\n",
       "      <td>75.13</td>\n",
       "      <td>77.20</td>\n",
       "      <td>78.62</td>\n",
       "      <td>61.46</td>\n",
       "      <td>80.76</td>\n",
       "      <td>77.76</td>\n",
       "      <td>77.07</td>\n",
       "      <td>73.37</td>\n",
       "      <td>79.25</td>\n",
       "      <td>75.16</td>\n",
       "      <td>81.36</td>\n",
       "      <td>69.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_300</th>\n",
       "      <td>82.52</td>\n",
       "      <td>64.32</td>\n",
       "      <td>81.72</td>\n",
       "      <td>75.00</td>\n",
       "      <td>82.45</td>\n",
       "      <td>80.94</td>\n",
       "      <td>81.43</td>\n",
       "      <td>73.01</td>\n",
       "      <td>82.62</td>\n",
       "      <td>78.58</td>\n",
       "      <td>78.68</td>\n",
       "      <td>77.69</td>\n",
       "      <td>82.61</td>\n",
       "      <td>81.85</td>\n",
       "      <td>83.23</td>\n",
       "      <td>75.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_400</th>\n",
       "      <td>78.07</td>\n",
       "      <td>65.90</td>\n",
       "      <td>79.76</td>\n",
       "      <td>76.92</td>\n",
       "      <td>81.16</td>\n",
       "      <td>80.17</td>\n",
       "      <td>82.15</td>\n",
       "      <td>74.00</td>\n",
       "      <td>82.49</td>\n",
       "      <td>79.86</td>\n",
       "      <td>79.69</td>\n",
       "      <td>79.60</td>\n",
       "      <td>81.57</td>\n",
       "      <td>81.98</td>\n",
       "      <td>81.58</td>\n",
       "      <td>77.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_Average</th>\n",
       "      <td>72.85</td>\n",
       "      <td>51.41</td>\n",
       "      <td>77.57</td>\n",
       "      <td>61.20</td>\n",
       "      <td>72.36</td>\n",
       "      <td>72.34</td>\n",
       "      <td>71.39</td>\n",
       "      <td>63.45</td>\n",
       "      <td>76.50</td>\n",
       "      <td>73.08</td>\n",
       "      <td>74.97</td>\n",
       "      <td>68.95</td>\n",
       "      <td>75.54</td>\n",
       "      <td>69.95</td>\n",
       "      <td>75.57</td>\n",
       "      <td>68.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_050</th>\n",
       "      <td>54.08</td>\n",
       "      <td>28.25</td>\n",
       "      <td>59.14</td>\n",
       "      <td>39.85</td>\n",
       "      <td>51.90</td>\n",
       "      <td>54.50</td>\n",
       "      <td>49.84</td>\n",
       "      <td>51.60</td>\n",
       "      <td>59.44</td>\n",
       "      <td>56.08</td>\n",
       "      <td>59.22</td>\n",
       "      <td>51.22</td>\n",
       "      <td>58.88</td>\n",
       "      <td>46.90</td>\n",
       "      <td>56.74</td>\n",
       "      <td>51.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_100</th>\n",
       "      <td>62.40</td>\n",
       "      <td>44.79</td>\n",
       "      <td>68.84</td>\n",
       "      <td>44.67</td>\n",
       "      <td>65.02</td>\n",
       "      <td>63.62</td>\n",
       "      <td>60.36</td>\n",
       "      <td>53.54</td>\n",
       "      <td>66.00</td>\n",
       "      <td>66.08</td>\n",
       "      <td>69.82</td>\n",
       "      <td>61.58</td>\n",
       "      <td>68.50</td>\n",
       "      <td>63.10</td>\n",
       "      <td>67.78</td>\n",
       "      <td>61.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_200</th>\n",
       "      <td>77.18</td>\n",
       "      <td>50.75</td>\n",
       "      <td>79.68</td>\n",
       "      <td>62.10</td>\n",
       "      <td>73.42</td>\n",
       "      <td>75.86</td>\n",
       "      <td>77.24</td>\n",
       "      <td>61.08</td>\n",
       "      <td>79.18</td>\n",
       "      <td>76.34</td>\n",
       "      <td>75.58</td>\n",
       "      <td>69.92</td>\n",
       "      <td>77.62</td>\n",
       "      <td>74.34</td>\n",
       "      <td>79.34</td>\n",
       "      <td>68.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_300</th>\n",
       "      <td>79.68</td>\n",
       "      <td>62.15</td>\n",
       "      <td>79.64</td>\n",
       "      <td>70.10</td>\n",
       "      <td>79.58</td>\n",
       "      <td>78.86</td>\n",
       "      <td>78.80</td>\n",
       "      <td>68.38</td>\n",
       "      <td>80.78</td>\n",
       "      <td>77.40</td>\n",
       "      <td>77.10</td>\n",
       "      <td>76.18</td>\n",
       "      <td>80.44</td>\n",
       "      <td>79.48</td>\n",
       "      <td>81.10</td>\n",
       "      <td>74.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_400</th>\n",
       "      <td>76.94</td>\n",
       "      <td>64.88</td>\n",
       "      <td>78.18</td>\n",
       "      <td>72.44</td>\n",
       "      <td>78.88</td>\n",
       "      <td>77.16</td>\n",
       "      <td>79.66</td>\n",
       "      <td>70.12</td>\n",
       "      <td>80.62</td>\n",
       "      <td>77.80</td>\n",
       "      <td>78.94</td>\n",
       "      <td>77.34</td>\n",
       "      <td>79.94</td>\n",
       "      <td>79.80</td>\n",
       "      <td>79.38</td>\n",
       "      <td>75.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_Average</th>\n",
       "      <td>70.06</td>\n",
       "      <td>50.17</td>\n",
       "      <td>73.10</td>\n",
       "      <td>57.83</td>\n",
       "      <td>69.76</td>\n",
       "      <td>70.00</td>\n",
       "      <td>69.18</td>\n",
       "      <td>60.94</td>\n",
       "      <td>73.20</td>\n",
       "      <td>70.74</td>\n",
       "      <td>72.13</td>\n",
       "      <td>67.25</td>\n",
       "      <td>73.08</td>\n",
       "      <td>68.72</td>\n",
       "      <td>72.87</td>\n",
       "      <td>66.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Araci_CM  Araci_NoPT  Araci_SM  Araci_WWM  BERT_CM  \\\n",
       "FinetuningSize_Measure                                                       \n",
       "f1-score_050               49.48       28.84     52.55      35.48    47.65   \n",
       "f1-score_100               59.77       44.39     65.34      40.29    63.09   \n",
       "f1-score_200               76.83       50.56     79.38      59.83    73.02   \n",
       "f1-score_300               79.25       61.43     79.32      67.50    79.12   \n",
       "f1-score_400               76.70       64.67     77.93      70.64    78.50   \n",
       "f1-score_Average           68.40       49.98     70.91      54.75    68.28   \n",
       "precision_050              58.22       30.29     69.26      43.14    54.03   \n",
       "precision_100              66.19       45.02     75.53      49.42    69.05   \n",
       "precision_200              79.22       51.53     81.59      61.54    75.13   \n",
       "precision_300              82.52       64.32     81.72      75.00    82.45   \n",
       "precision_400              78.07       65.90     79.76      76.92    81.16   \n",
       "precision_Average          72.85       51.41     77.57      61.20    72.36   \n",
       "recall_050                 54.08       28.25     59.14      39.85    51.90   \n",
       "recall_100                 62.40       44.79     68.84      44.67    65.02   \n",
       "recall_200                 77.18       50.75     79.68      62.10    73.42   \n",
       "recall_300                 79.68       62.15     79.64      70.10    79.58   \n",
       "recall_400                 76.94       64.88     78.18      72.44    78.88   \n",
       "recall_Average             70.06       50.17     73.10      57.83    69.76   \n",
       "\n",
       "                        BERT_NoPT  BERT_SM  BERT_WWM  SECBERT_CM  \\\n",
       "FinetuningSize_Measure                                             \n",
       "f1-score_050                49.15    42.11     49.86       53.65   \n",
       "f1-score_100                62.54    56.34     51.50       62.35   \n",
       "f1-score_200                75.58    76.98     58.94       78.94   \n",
       "f1-score_300                78.51    78.37     65.91       80.52   \n",
       "f1-score_400                76.60    79.26     68.60       80.34   \n",
       "f1-score_Average            68.48    66.61     58.96       71.16   \n",
       "precision_050               57.79    46.99     54.18       62.53   \n",
       "precision_100               65.59    67.76     54.61       74.10   \n",
       "precision_200               77.20    78.62     61.46       80.76   \n",
       "precision_300               80.94    81.43     73.01       82.62   \n",
       "precision_400               80.17    82.15     74.00       82.49   \n",
       "precision_Average           72.34    71.39     63.45       76.50   \n",
       "recall_050                  54.50    49.84     51.60       59.44   \n",
       "recall_100                  63.62    60.36     53.54       66.00   \n",
       "recall_200                  75.86    77.24     61.08       79.18   \n",
       "recall_300                  78.86    78.80     68.38       80.78   \n",
       "recall_400                  77.16    79.66     70.12       80.62   \n",
       "recall_Average              70.00    69.18     60.94       73.20   \n",
       "\n",
       "                        SECBERT_NoPT  SECBERT_SM  SECBERT_WWM  Yang_CM  \\\n",
       "FinetuningSize_Measure                                                   \n",
       "f1-score_050                   50.84       53.45        45.57    55.05   \n",
       "f1-score_100                   64.93       68.54        60.38    67.57   \n",
       "f1-score_200                   76.07       75.23        68.33    77.28   \n",
       "f1-score_300                   77.19       76.84        75.89    80.12   \n",
       "f1-score_400                   77.43       78.82        76.87    79.69   \n",
       "f1-score_Average               69.29       70.57        65.41    71.94   \n",
       "precision_050                  60.62       65.69        51.65    63.57   \n",
       "precision_100                  68.58       73.75        62.46    70.67   \n",
       "precision_200                  77.76       77.07        73.37    79.25   \n",
       "precision_300                  78.58       78.68        77.69    82.61   \n",
       "precision_400                  79.86       79.69        79.60    81.57   \n",
       "precision_Average              73.08       74.97        68.95    75.54   \n",
       "recall_050                     56.08       59.22        51.22    58.88   \n",
       "recall_100                     66.08       69.82        61.58    68.50   \n",
       "recall_200                     76.34       75.58        69.92    77.62   \n",
       "recall_300                     77.40       77.10        76.18    80.44   \n",
       "recall_400                     77.80       78.94        77.34    79.94   \n",
       "recall_Average                 70.74       72.13        67.25    73.08   \n",
       "\n",
       "                        Yang_NoPT  Yang_SM  Yang_WWM  \n",
       "FinetuningSize_Measure                                \n",
       "f1-score_050                42.59    51.43     43.65  \n",
       "f1-score_100                62.33    66.55     59.98  \n",
       "f1-score_200                74.16    79.03     68.04  \n",
       "f1-score_300                79.07    80.81     74.43  \n",
       "f1-score_400                79.43    79.02     75.22  \n",
       "f1-score_Average            67.52    71.37     64.26  \n",
       "precision_050               47.05    60.94     55.96  \n",
       "precision_100               63.69    70.72     65.04  \n",
       "precision_200               75.16    81.36     69.94  \n",
       "precision_300               81.85    83.23     75.74  \n",
       "precision_400               81.98    81.58     77.54  \n",
       "precision_Average           69.95    75.57     68.84  \n",
       "recall_050                  46.90    56.74     51.52  \n",
       "recall_100                  63.10    67.78     61.86  \n",
       "recall_200                  74.34    79.34     68.80  \n",
       "recall_300                  79.48    81.10     74.68  \n",
       "recall_400                  79.80    79.38     75.70  \n",
       "recall_Average              68.72    72.87     66.51  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs1, dfs2, dfs3 = [], [], []\n",
    "for dirname in tqdm(dirnames):\n",
    "    test_company, model_name, method_name, epoch_num, seed_num = os.path.basename(dirname).split('_')\n",
    "    \n",
    "    records1, records2, records3 = [], [], []\n",
    "    for target_trained_with_num in target_trained_with_nums:\n",
    "        filepaths = glob(os.path.join(os.path.dirname(dirname), '{}_*'.format('_'.join([test_company, model_name, method_name, epoch_num])), \\\n",
    "                  'classification_report_train_{}.csv'.format(target_trained_with_num)))\n",
    "        \n",
    "        averaged_classification_report_df = get_average_classification_report([pd.read_csv(filepath) for filepath \\\n",
    "                                                                               in filepaths])\n",
    "        item = averaged_classification_report_df.loc['macro avg']\n",
    "        records1.append(('precision_{:03d}'.format(target_trained_with_num), 100*item['precision']))\n",
    "        records2.append(('recall_{:03d}'.format(target_trained_with_num), 100*item['recall']))\n",
    "        records3.append(('f1-score_{:03d}'.format(target_trained_with_num), 100*item['f1-score']))\n",
    "    dfs1.append(pd.DataFrame(records1, columns=['FinetuningSize_Measure', '{}_{}'.format(model_name, method_name)]))\n",
    "    dfs2.append(pd.DataFrame(records2, columns=['FinetuningSize_Measure', '{}_{}'.format(model_name, method_name)]))\n",
    "    dfs3.append(pd.DataFrame(records3, columns=['FinetuningSize_Measure', '{}_{}'.format(model_name, method_name)]))\n",
    "\n",
    "result1 = reduce(lambda df1,df2: pd.merge(df1,df2,on='FinetuningSize_Measure'), dfs1)\n",
    "result1.set_index('FinetuningSize_Measure', inplace=True)\n",
    "result1.loc['precision_Average'] = result1.mean()\n",
    "result2 = reduce(lambda df1,df2: pd.merge(df1,df2,on='FinetuningSize_Measure'), dfs2)\n",
    "result2.set_index('FinetuningSize_Measure', inplace=True)\n",
    "result2.loc['recall_Average'] = result2.mean()\n",
    "result3 = reduce(lambda df1,df2: pd.merge(df1,df2,on='FinetuningSize_Measure'), dfs3)\n",
    "result3.set_index('FinetuningSize_Measure', inplace=True)\n",
    "result3.loc['f1-score_Average'] = result3.mean()\n",
    "\n",
    "result = pd.concat([result1, result2, result3])\n",
    "result.sort_values(by=['FinetuningSize_Measure'], inplace=True)\n",
    "\n",
    "filepath = os.path.join('results', 'LowResource_TestSetContainingCompanyNames_PrecisionRecallF1score.csv')\n",
    "if not os.path.exists(os.path.dirname(filepath)): os.makedirs(os.path.dirname(filepath))\n",
    "result.to_csv(filepath, index=False)\n",
    "print('Created {}'.format(filepath))\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
